{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T15:51:58.503006Z",
     "start_time": "2020-08-01T15:51:58.469381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity = \"all\"\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T15:51:59.749827Z",
     "start_time": "2020-08-01T15:51:59.079304Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T15:52:06.458186Z",
     "start_time": "2020-08-01T15:51:59.794034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train_set.csv\", sep = \"\\t\", index_col = False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T15:52:12.108164Z",
     "start_time": "2020-08-01T15:52:12.084575Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用于评论的分词\n",
    "class SegmentVocab(object):\n",
    "    \"\"\"\n",
    "    参数/属性 data:需要分词的数据集\n",
    "    参数 class_sentence_seg_dict：按类划分及按数据集划分的词典--->{'class1':[[seg1, seg2], [seg3, ..]]}\n",
    "    参数 vocab_dict: 整个词表\n",
    "    参数 class_seg_dict： {'class1':[seg1, seg2, ...]}\n",
    "    参数 common_vocab: 公共词（区分度不高的词）\n",
    "    \"\"\"\n",
    "    def __init__(self, data, min_seq):\n",
    "        self.data = data\n",
    "        self.class_sentence_seg_dict, self.vocab_dict, self.class_seg_dict \\\n",
    "            = self.class_segment(min_seq)\n",
    "        self.common_vocab = self.get_common_vocab()\n",
    "\n",
    "    def class_segment(self, min_seq):\n",
    "        \"\"\"\n",
    "        功能：按类别进行分词/去停词/过滤低频词\n",
    "        参数 min_seq:指定低频词的阈值, 用于过滤低频词\n",
    "        返回：按类保存的分词结果的分词字典\n",
    "        \"\"\"\n",
    "        # 存储留言类别（唯一的）\n",
    "        categories = self.data['label'].unique()\n",
    "        # 按类存储分词结果\n",
    "        vocab = {}\n",
    "\n",
    "        # 按类获取sentence列表\n",
    "        for category in categories:\n",
    "            # 提取评价\n",
    "            data_class = self.data.loc[self.data['label'] == category, ['text']]\n",
    "            sentence_list = data_class['text'].values.tolist()\n",
    "            # 获得词典\n",
    "            vocab[category] = sentence_list\n",
    "\n",
    "        # 按类进行分词\n",
    "        print(\"segmentation start...\")\n",
    "        # vocab_lcut为{'class1':[[seg1, seg2], [seg3, ..]]}\n",
    "        # vocab_lcut_sum为{'class1':[seg1, seg2, ...]}\n",
    "        # words_dict是整个词表\n",
    "        vocab_lcut, vocab_lcut_sum, words_dict = {}, {}, {}\n",
    "        for key, value in tqdm(vocab.items()):\n",
    "            segment1, segment2 = [], []\n",
    "            for line in value:\n",
    "                try:\n",
    "                    segs = line.split(\" \")\n",
    "                    segment1.append(segs)\n",
    "                    # 统计每个词出现的频率, 之后用于过滤低频词以及作为整个词表\n",
    "                    for seg in segs:\n",
    "                        words_dict[seg] = words_dict.get(seg, 0) + 1\n",
    "                        segment2.append(seg)\n",
    "                except:\n",
    "                    print(line)\n",
    "                    continue\n",
    "            vocab_lcut[key] = segment1\n",
    "            vocab_lcut_sum[key] = segment2\n",
    "        print(\"segmentation finish...\\n\")\n",
    "\n",
    "        # 过滤低频词\n",
    "        if min_seq > 0:\n",
    "            print(\"filter low-tf vocab start...\")\n",
    "            high_tf_dict, high_tf_sum_dict = {}, {}\n",
    "            for key, value in tqdm(vocab_lcut.items()):\n",
    "                high_tf_list, high_tf_sum_list = [], []\n",
    "                for line in value:\n",
    "                    line = list(filter(lambda x: words_dict[x] > min_seq, line))\n",
    "                    if len(line) > 0:\n",
    "                        high_tf_list.append(line)\n",
    "                        high_tf_sum_list.extend(line)\n",
    "                high_tf_dict[key] = high_tf_list\n",
    "                high_tf_sum_dict[key] = high_tf_sum_dict\n",
    "\n",
    "            vocab_lcut = high_tf_dict\n",
    "            vocab_lcut_sum = high_tf_sum_dict\n",
    "            print(\"filter low-tf vocab finish...\\n\")\n",
    "\n",
    "        return vocab_lcut, words_dict, vocab_lcut_sum\n",
    "    \n",
    "    def get_common_vocab(self):\n",
    "        \"\"\"\n",
    "        功能：获取各类的共有词（这类词的区分度不大, 建立特征时需要丢弃这些词）\n",
    "        返回：共有词表\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        common_set = set()\n",
    "        for key in self.class_seg_dict.keys():\n",
    "            if count == 0:\n",
    "                common_set = set(self.class_seg_dict[key])\n",
    "            else:\n",
    "                common_set = common_set.intersection(set(self.class_seg_dict[key]))\n",
    "            count += 1\n",
    "        return common_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-01T15:52:12.864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac365dabd22a4703ba6628cf3cd200f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seg_vocab = SegmentVocab(train_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
